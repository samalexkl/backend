{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36bbd700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/safernandez/Fk-Diffusion-Steering/text_to_image\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cd Fk-Diffusion-Steering/text_to_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f380521a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 23:49:10.157057: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/safernandez/.local/lib/python3.8/site-packages/transformers/models/auto/image_processing_auto.py:520: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Servidor público: NgrokTunnel: \"https://08c8-193-147-26-250.ngrok-free.app\" -> \"http://localhost:8000\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [1526632]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5398547c69e4441383c6551e0f2120c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1], [2, 2]]\n",
      "Args: {'lmbda': 8.0, 'num_particles': 4, 'adaptive_resampling': True, 'resample_frequency': 10, 'time_steps': 50, 'potential_type': 'max', 'resampling_t_start': 10, 'resampling_t_end': 50, 'guidance_reward_fn': 'ImageReward', 'use_smc': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2870b68e0624ed4841f5a9234f43b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`metric_to_chase` will be ignored as it only applies to 'LLMGrader' as the `reward_name`\n",
      "load checkpoint from /home/safernandez/.cache/ImageReward/ImageReward.pt\n",
      "checkpoint loaded\n",
      "`metric_to_chase` will be ignored as it only applies to 'LLMGrader' as the `reward_name`\n",
      "`metric_to_chase` will be ignored as it only applies to 'LLMGrader' as the `reward_name`\n",
      "`metric_to_chase` will be ignored as it only applies to 'LLMGrader' as the `reward_name`\n",
      "`metric_to_chase` will be ignored as it only applies to 'LLMGrader' as the `reward_name`\n",
      "Args: {'lmbda': 8.0, 'num_particles': 4, 'adaptive_resampling': True, 'resample_frequency': 10, 'time_steps': 50, 'potential_type': 'max', 'resampling_t_start': 10, 'resampling_t_end': 50, 'guidance_reward_fn': 'ImageReward', 'use_smc': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a74be567afa4b1ead701a9ad9ac30fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`metric_to_chase` will be ignored as it only applies to 'LLMGrader' as the `reward_name`\n",
      "`metric_to_chase` will be ignored as it only applies to 'LLMGrader' as the `reward_name`\n",
      "`metric_to_chase` will be ignored as it only applies to 'LLMGrader' as the `reward_name`\n",
      "`metric_to_chase` will be ignored as it only applies to 'LLMGrader' as the `reward_name`\n",
      "`metric_to_chase` will be ignored as it only applies to 'LLMGrader' as the `reward_name`\n",
      "Args: {'lmbda': 8.0, 'num_particles': 4, 'adaptive_resampling': True, 'resample_frequency': 10, 'time_steps': 50, 'potential_type': 'max', 'resampling_t_start': 10, 'resampling_t_end': 50, 'guidance_reward_fn': 'ImageReward', 'use_smc': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d2e84a05e14b299531343775ae59ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`metric_to_chase` will be ignored as it only applies to 'LLMGrader' as the `reward_name`\n",
      "`metric_to_chase` will be ignored as it only applies to 'LLMGrader' as the `reward_name`\n",
      "`metric_to_chase` will be ignored as it only applies to 'LLMGrader' as the `reward_name`\n",
      "`metric_to_chase` will be ignored as it only applies to 'LLMGrader' as the `reward_name`\n",
      "`metric_to_chase` will be ignored as it only applies to 'LLMGrader' as the `reward_name`\n",
      "INFO:     94.76.151.213:0 - \"POST /generate_custom HTTP/1.1\" 200 OK\n",
      "INFO:     94.76.151.213:0 - \"GET /files/image_1.png HTTP/1.1\" 200 OK\n",
      "INFO:     94.76.151.213:0 - \"GET /files/image_2.png HTTP/1.1\" 200 OK\n",
      "INFO:     94.76.151.213:0 - \"GET /files/image_3.png HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from fastapi import FastAPI\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from pyngrok import ngrok\n",
    "import torch\n",
    "from diffusers import LCMScheduler, AutoPipelineForText2Image\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "from transformers import AutoModelForCausalLM\n",
    "from PIL import Image\n",
    "import uuid, os\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('fkd_diffusers')\n",
    "from fastapi import FastAPI\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from pydantic import BaseModel\n",
    "from pyngrok import ngrok\n",
    "import torch, argparse, uuid, os\n",
    "from datetime import datetime\n",
    "from fks_utils import get_model\n",
    "from diffusers import LCMScheduler, AutoPipelineForText2Image, StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from typing import List, Optional\n",
    "import torch\n",
    "import requests\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms as tfms\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "\n",
    "\n",
    "# Useful function for later\n",
    "def load_image(url, size=None):\n",
    "    response = requests.get(url, timeout=0.2)\n",
    "    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    if size is not None:\n",
    "        img = img.resize(size)\n",
    "    return img\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Set up a DDIM scheduler\n",
    "\n",
    "# Sample function (regular DDIM)\n",
    "@torch.no_grad()\n",
    "def sample(\n",
    "    prompt,\n",
    "    start_step=0,\n",
    "    start_latents=None,\n",
    "    guidance_scale=3.5,\n",
    "    num_inference_steps=30,\n",
    "    num_images_per_prompt=1,\n",
    "    do_classifier_free_guidance=True,\n",
    "    negative_prompt=\"\",\n",
    "    device=device,\n",
    "):\n",
    "\n",
    "    # Encode prompt\n",
    "    text_embeddings = pipe._encode_prompt(\n",
    "        prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n",
    "    )\n",
    "\n",
    "    # Set num inference steps\n",
    "    pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "\n",
    "    # Create a random starting point if we don't have one already\n",
    "    if start_latents is None:\n",
    "        start_latents = torch.randn(1, 4, 64, 64, device=device)\n",
    "        start_latents *= pipe.scheduler.init_noise_sigma\n",
    "\n",
    "    latents = start_latents.clone()\n",
    "\n",
    "    for i in tqdm(range(start_step, num_inference_steps)):\n",
    "\n",
    "        t = pipe.scheduler.timesteps[i]\n",
    "\n",
    "        # Expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "        latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "        # Predict the noise residual\n",
    "        noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "        # Perform guidance\n",
    "        if do_classifier_free_guidance:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        # Normally we'd rely on the scheduler to handle the update step:\n",
    "        # latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "        # Instead, let's do it ourselves:\n",
    "        prev_t = max(1, t.item() - (1000 // num_inference_steps))  # t-1\n",
    "        alpha_t = pipe.scheduler.alphas_cumprod[t.item()]\n",
    "        alpha_t_prev = pipe.scheduler.alphas_cumprod[prev_t]\n",
    "        predicted_x0 = (latents - (1 - alpha_t).sqrt() * noise_pred) / alpha_t.sqrt()\n",
    "        direction_pointing_to_xt = (1 - alpha_t_prev).sqrt() * noise_pred\n",
    "        latents = alpha_t_prev.sqrt() * predicted_x0 + direction_pointing_to_xt\n",
    "\n",
    "    # Post-processing\n",
    "    images = pipe.decode_latents(latents)\n",
    "    images = pipe.numpy_to_pil(images)\n",
    "\n",
    "    return images\n",
    "# https://www.pexels.com/photo/a-beagle-on-green-grass-field-8306128/\n",
    "\n",
    "# Encode with VAE\n",
    "\n",
    "## Inversion\n",
    "@torch.no_grad()\n",
    "def invert(\n",
    "    start_latents,\n",
    "    prompt,\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=80,\n",
    "    num_images_per_prompt=1,\n",
    "    do_classifier_free_guidance=True,\n",
    "    negative_prompt=\"\",\n",
    "    device=device,\n",
    "    pipe=None,\n",
    "):\n",
    "\n",
    "    # Encode prompt\n",
    "    text_embeddings = pipe._encode_prompt(\n",
    "        prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n",
    "    )\n",
    "\n",
    "    # Latents are now the specified start latents\n",
    "    latents = start_latents.clone()\n",
    "\n",
    "    # We'll keep a list of the inverted latents as the process goes on\n",
    "    intermediate_latents = []\n",
    "\n",
    "    # Set num inference steps\n",
    "    pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "\n",
    "    # Reversed timesteps <<<<<<<<<<<<<<<<<<<<\n",
    "    timesteps = reversed(pipe.scheduler.timesteps)\n",
    "\n",
    "    for i in tqdm(range(1, num_inference_steps), total=num_inference_steps - 1):\n",
    "\n",
    "        # We'll skip the final iteration\n",
    "        if i >= num_inference_steps - 1:\n",
    "            continue\n",
    "\n",
    "        t = timesteps[i]\n",
    "\n",
    "        # Expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "        latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "        # Predict the noise residual\n",
    "        noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "        # Perform guidance\n",
    "        if do_classifier_free_guidance:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        current_t = max(0, t.item() - (1000 // num_inference_steps))  # t\n",
    "        next_t = t  # min(999, t.item() + (1000//num_inference_steps)) # t+1\n",
    "        alpha_t = pipe.scheduler.alphas_cumprod[current_t]\n",
    "        alpha_t_next = pipe.scheduler.alphas_cumprod[next_t]\n",
    "\n",
    "        # Inverted update step (re-arranging the update step to get x(t) (new latents) as a function of x(t-1) (current latents)\n",
    "        latents = (latents - (1 - alpha_t).sqrt() * noise_pred) * (alpha_t_next.sqrt() / alpha_t.sqrt()) + (\n",
    "            1 - alpha_t_next\n",
    "        ).sqrt() * noise_pred\n",
    "\n",
    "        # Store\n",
    "        intermediate_latents.append(latents)\n",
    "\n",
    "    return torch.cat(intermediate_latents)\n",
    "\n",
    "\n",
    "#   carpeta de imágenes\n",
    "os.makedirs(\"generated_images\", exist_ok=True)\n",
    "\n",
    "# 3. FastAPI init\n",
    "app = FastAPI()\n",
    "app.mount(\"/files\", StaticFiles(directory=\"generated_images\"), name=\"files\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# ----------------------\n",
    "# Inicializar el backend\n",
    "# ----------------------\n",
    "app = FastAPI()\n",
    "nest_asyncio.apply()\n",
    "app.mount(\n",
    "    \"/files\",\n",
    "    StaticFiles(directory=\"generated_images\"),\n",
    "    name=\"files\"\n",
    ")\n",
    "\n",
    "\n",
    "class PromptRequest(BaseModel):\n",
    "    prompts: List[str]  \n",
    "    question: str       \n",
    "    numbers: Optional[List[int]] = None \n",
    "       \n",
    "num_inference_steps = 100\n",
    "guidance_scale = 7.5\n",
    "attn_res = (16, 16)\n",
    "steps_to_save_attention_maps = list(range(num_inference_steps))\n",
    "max_iter_to_alter = 50\n",
    "refinement_steps = 100\n",
    "scale_factor = 30\n",
    "iterative_refinement_steps = [0, 10, 20]\n",
    "do_smoothing = True\n",
    "smoothing_sigma = 0.2\n",
    "smoothing_kernel_size = 3\n",
    "temperature = 0.5\n",
    "softmax_normalize = False\n",
    "softmax_normalize_attention_maps = False\n",
    "add_previous_attention_maps = True\n",
    "previous_attention_map_anchor_step = None\n",
    "loss_fn = \"ntxent\"\n",
    "\n",
    "\n",
    "# Args  get_model\n",
    "args = dict(\n",
    "    \n",
    "    output_dir=\"output\",\n",
    "    eta=1.0,\n",
    "    metrics_to_compute=\"ImageReward\",\n",
    "    prompt_path=\"\",  \n",
    "    model_name=\"stable-diffusion-v1-5\",\n",
    ")\n",
    "fkd_args = dict(\n",
    "    lmbda=8.0,\n",
    "    num_particles=4,\n",
    "    adaptive_resampling=True,\n",
    "    resample_frequency=10,\n",
    "    time_steps=50,\n",
    "    potential_type='max',\n",
    "    resampling_t_start=10,\n",
    "    resampling_t_end=50,\n",
    "    guidance_reward_fn='ImageReward',\n",
    "    use_smc=True,\n",
    ")\n",
    "\n",
    "ns = {**args, **fkd_args}\n",
    "all_args = argparse.Namespace(**ns)\n",
    "all_args.num_inference_steps = fkd_args[\"time_steps\"]\n",
    "\n",
    "\n",
    "\n",
    "# 6. Endpoint\n",
    "@app.post(\"/generate_custom\")\n",
    "async def generate_custom(req: PromptRequest):\n",
    "    pipe = get_model(all_args.model_name)\n",
    "    pipe = pipe.to(\"cuda\")\n",
    "    adapter_id = \"latent-consistency/lcm-lora-sdv1-5\"\n",
    "    pipe.load_lora_weights(adapter_id)\n",
    "    pipe.fuse_lora()\n",
    "    guidance_scale = 0\n",
    "    \n",
    "    #Para Latent consistency en fk conform:\n",
    "    args = dict(\n",
    "    \n",
    "    output_dir=\"output\",\n",
    "    eta=1.0,\n",
    "    metrics_to_compute=\"ImageReward\",\n",
    "    prompt_path=\"\",  \n",
    "    model_name=\"stable-diffusion-v1-5\",\n",
    ")\n",
    "    fkd_args = dict(\n",
    "        lmbda=8.0,\n",
    "        num_particles=4,\n",
    "        adaptive_resampling=True,\n",
    "        resample_frequency=10,\n",
    "        time_steps=30,\n",
    "        potential_type='max',\n",
    "        resampling_t_start=10,\n",
    "        resampling_t_end=50,\n",
    "        guidance_reward_fn='ImageReward',\n",
    "        use_smc=True,\n",
    "    )\n",
    "\n",
    "    ns = {**args, **fkd_args}\n",
    "    all_args = argparse.Namespace(**ns)\n",
    "    all_args.num_inference_steps = fkd_args[\"time_steps\"]\n",
    "\n",
    "    \n",
    "    prompts = req.prompts[:3]\n",
    "    question = req.question\n",
    "    image_paths = []\n",
    "    answers = []\n",
    "    scenes = [\"first scene, tell the story like if its a movie\", \"second scene, talk like if you are a pirate\", \"final scene, talk like a storyteller\"]\n",
    "    prompts = [\n",
    "    {\"prompt\": prompts[0]},\n",
    "    {\"prompt\": prompts[1]},\n",
    "    {\"prompt\": prompts[2]},\n",
    "    \n",
    "]\n",
    "    all_numbers = req.numbers\n",
    "            \n",
    "           \n",
    "    token_groups=[all_numbers[0:2], all_numbers[2:4]]\n",
    "    print(token_groups)\n",
    "    token_groups2=[all_numbers[4:6], all_numbers[6:8]]\n",
    "    token_groups3=[all_numbers[8:10], all_numbers[10:12]]\n",
    "    \n",
    "    \n",
    "    for i, p in enumerate(prompts):\n",
    "        prompt=[p['prompt']]*fkd_args['num_particles']\n",
    "        if i==1:\n",
    "            token_groups = token_groups2\n",
    "        if i == 2:\n",
    "            token_groups = token_groups3\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            token_groups=token_groups,  \n",
    "            fkd_args=fkd_args,\n",
    "            num_inference_steps=fkd_args[\"time_steps\"],\n",
    "            eta=all_args.eta,\n",
    "            guidance_scale=guidance_scale,\n",
    "            \n",
    "            max_iter_to_alter=max_iter_to_alter,\n",
    "            attn_res=attn_res,\n",
    "            scale_factor=scale_factor,\n",
    "            iterative_refinement_steps=iterative_refinement_steps,\n",
    "            steps_to_save_attention_maps=steps_to_save_attention_maps,\n",
    "            do_smoothing=do_smoothing,\n",
    "            smoothing_sigma=smoothing_sigma,\n",
    "            smoothing_kernel_size=smoothing_kernel_size,\n",
    "            temperature=temperature,\n",
    "            refinement_steps=refinement_steps,\n",
    "            softmax_normalize=softmax_normalize,\n",
    "            softmax_normalize_attention_maps=softmax_normalize_attention_maps,\n",
    "            add_previous_attention_maps=add_previous_attention_maps,\n",
    "            previous_attention_map_anchor_step=previous_attention_map_anchor_step,\n",
    "            loss_fn=loss_fn,\n",
    "        )[0] \n",
    "        filename = f\"generated_images/image_{i + 1}.png\"  \n",
    "        image[0].save(filename)\n",
    "        image_paths.append(filename)\n",
    "    del pipe   \n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "        \n",
    "    janus_path = \"deepseek-ai/Janus-Pro-1B\"\n",
    "    vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(janus_path)\n",
    "    tokenizer = vl_chat_processor.tokenizer\n",
    "    vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(janus_path, trust_remote_code=True)\n",
    "    vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        question += scenes[i] + \"\\n\"\n",
    "        \n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"<|User|>\",\n",
    "                \"content\": f\"<image_placeholder>\\n{question}\",\n",
    "                \"images\": [img_path],   \n",
    "            },\n",
    "            {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n",
    "        ]\n",
    "        \n",
    "        pil_images = load_pil_images(conversation)\n",
    "\n",
    "       \n",
    "        prepare_inputs = vl_chat_processor(\n",
    "            conversations=conversation,\n",
    "            images=pil_images,\n",
    "            force_batchify=True\n",
    "        ).to(vl_gpt.device)\n",
    "        inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "      \n",
    "        output = vl_gpt.language_model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=prepare_inputs.attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "        )[0]\n",
    "\n",
    "        # Decodificamos y almacenamos\n",
    "        text = tokenizer.decode(output.cpu().tolist(), skip_special_tokens=True)\n",
    "        answers.append(text)\n",
    "\n",
    "    return {\n",
    "        \"message\": \"Proceso completado\",\n",
    "        \"generated_images\": image_paths,\n",
    "        \"answers\": answers     # lista de 3 strings\n",
    "    }\n",
    "\n",
    "@app.post(\"/generate_full\")\n",
    "async def generate_full(req: PromptRequest):\n",
    "    \n",
    "    \n",
    "    model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "    adapter_id = \"latent-consistency/lcm-lora-sdxl\"\n",
    "    pipe = AutoPipelineForText2Image.from_pretrained(model_id, torch_dtype=torch.float16, variant=\"fp16\")\n",
    "    pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "    pipe.to(\"cuda\")\n",
    "    pipe.load_lora_weights(adapter_id)\n",
    "    pipe.fuse_lora()\n",
    "\n",
    "    prompts = req.prompts[:3]\n",
    "    question = req.question\n",
    "    image_paths = []\n",
    "    answers = []\n",
    "    scenes = [\"first scene\", \"second scene\", \"final scene\"]\n",
    "   \n",
    "    for i, p in enumerate(prompts[:3]):\n",
    "        image = pipe(prompt=p, num_inference_steps=10, guidance_scale=0).images[0]\n",
    "        filename = f\"generated_images/image_{i + 5}.png\"  \n",
    "        image.save(filename)\n",
    "        image_paths.append(filename)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    del pipe   \n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    prompts = req.prompts[:3]\n",
    "    question = req.question\n",
    "    image_paths = []\n",
    "    answers = []\n",
    "    scenes = [\"first scene, tell the story like if its a movie\", \"second scene, talk like if you are a pirate\", \"final scene, talk like a storyteller\"]\n",
    "    prompts = [\n",
    "        {\"prompt\": prompts[0]},\n",
    "        {\"prompt\": prompts[1]},\n",
    "        {\"prompt\": prompts[2]},\n",
    "\n",
    "    ]\n",
    "    all_numbers = req.numbers\n",
    "\n",
    "\n",
    "    token_groups=[all_numbers[0:2], all_numbers[2:4]]\n",
    "    print(token_groups)\n",
    "    token_groups2=[all_numbers[4:6], all_numbers[6:8]]\n",
    "    token_groups3=[all_numbers[8:10], all_numbers[10:12]]\n",
    "    pipe = get_model(all_args.model_name)\n",
    "    pipe = pipe.to(\"cuda\")\n",
    "    for i, p in enumerate(prompts):\n",
    "        \n",
    "        prompt=[p['prompt']]*fkd_args['num_particles']\n",
    "        if i==1:\n",
    "            token_groups = token_groups2\n",
    "        if i == 2:\n",
    "            token_groups = token_groups3\n",
    "\n",
    "\n",
    "        input_image = Image.open(f\"generated_images/image_{i + 5}.png\").convert(\"RGB\")\n",
    "\n",
    "    \n",
    "        input_image = input_image.resize((512, 512))\n",
    "        input_image_prompt = p['prompt']\n",
    "        with torch.no_grad():\n",
    "\n",
    "            input_tensor = tfms.functional.to_tensor(input_image).unsqueeze(0).to(device).to(torch.float16)\n",
    "\n",
    "              # Asegúrate de escalar a [-1, 1] si eso espera tu VAE\n",
    "            input_tensor = input_tensor * 2 - 1\n",
    "\n",
    "              # Codificar usando el VAE\n",
    "            latent = pipe.vae.encode(input_tensor)\n",
    "\n",
    "            # Samplear y escalar latente\n",
    "        l = 0.18215 * latent.latent_dist.sample()\n",
    "\n",
    "            # Inversión (asumiendo que esta función está bien definida)\n",
    "        inverted_latents = invert(l, input_image_prompt, num_inference_steps=50,pipe= pipe)    \n",
    "        image = pipe(\n",
    "                prompt=prompt,\n",
    "                latents=inverted_latents[-(20 + 1)][None],\n",
    "                token_groups=token_groups,  \n",
    "                fkd_args=fkd_args,\n",
    "                num_inference_steps=fkd_args[\"time_steps\"],\n",
    "                eta=all_args.eta,\n",
    "                guidance_scale=guidance_scale,\n",
    "                start_step=20,\n",
    "                max_iter_to_alter=max_iter_to_alter,\n",
    "                attn_res=attn_res,\n",
    "                scale_factor=scale_factor,\n",
    "                iterative_refinement_steps=iterative_refinement_steps,\n",
    "                steps_to_save_attention_maps=steps_to_save_attention_maps,\n",
    "                do_smoothing=do_smoothing,\n",
    "                smoothing_sigma=smoothing_sigma,\n",
    "                smoothing_kernel_size=smoothing_kernel_size,\n",
    "                temperature=temperature,\n",
    "                refinement_steps=refinement_steps,\n",
    "                softmax_normalize=softmax_normalize,\n",
    "                softmax_normalize_attention_maps=softmax_normalize_attention_maps,\n",
    "                add_previous_attention_maps=add_previous_attention_maps,\n",
    "                previous_attention_map_anchor_step=previous_attention_map_anchor_step,\n",
    "                loss_fn=loss_fn,\n",
    "            )[0] \n",
    "        filename = f\"generated_images/image_{i + 1}.png\"  \n",
    "        image[0].save(filename)\n",
    "        image_paths.append(filename)\n",
    "    del pipe   \n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "    janus_path = \"deepseek-ai/Janus-Pro-1B\"\n",
    "    vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(janus_path)\n",
    "    tokenizer = vl_chat_processor.tokenizer\n",
    "    vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(janus_path, trust_remote_code=True)\n",
    "    vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "    for i, img_path in enumerate(image_paths[:3]):\n",
    "        question += scenes[i] + \"\\n\"\n",
    "\n",
    "        conversation = [\n",
    "                {\n",
    "                    \"role\": \"<|User|>\",\n",
    "                    \"content\": f\"<image_placeholder>\\n{question}\",\n",
    "                    \"images\": [img_path],   \n",
    "                },\n",
    "                {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n",
    "            ]\n",
    "\n",
    "        pil_images = load_pil_images(conversation)\n",
    "\n",
    "\n",
    "        prepare_inputs = vl_chat_processor(\n",
    "                conversations=conversation,\n",
    "                images=pil_images,\n",
    "                force_batchify=True\n",
    "            ).to(vl_gpt.device)\n",
    "        inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "\n",
    "        output = vl_gpt.language_model.generate(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=prepare_inputs.attention_mask,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                bos_token_id=tokenizer.bos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=False,\n",
    "                use_cache=True,\n",
    "            )[0]\n",
    "\n",
    "            # Decodificamos y almacenamos\n",
    "        text = tokenizer.decode(output.cpu().tolist(), skip_special_tokens=True)\n",
    "        answers.append(text)\n",
    "\n",
    "    return {\n",
    "            \"message\": \"Proceso completado\",\n",
    "            \"generated_images\": image_paths,\n",
    "            \"answers\": answers     # lista de 3 strings\n",
    "        }\n",
    "\n",
    "# Modelo Janus\n",
    "\n",
    "\n",
    "# Carpeta para guardar imágenes\n",
    "os.makedirs(\"generated_images\", exist_ok=True)\n",
    "\n",
    "# ----------------------\n",
    "# Endpoint principal\n",
    "# ----------------------\n",
    "\n",
    "@app.post(\"/generate_and_answer\")\n",
    "async def generate_and_answer(req: PromptRequest):\n",
    "    model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "    adapter_id = \"latent-consistency/lcm-lora-sdxl\"\n",
    "    pipe = AutoPipelineForText2Image.from_pretrained(model_id, torch_dtype=torch.float16, variant=\"fp16\")\n",
    "    pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "    pipe.to(\"cuda\")\n",
    "    pipe.load_lora_weights(adapter_id)\n",
    "    pipe.fuse_lora()\n",
    "\n",
    "    prompts = req.prompts[:3]\n",
    "    question = req.question\n",
    "    image_paths = []\n",
    "    answers = []\n",
    "    scenes = [\"first scene\", \"second scene\", \"final scene\"]\n",
    "   \n",
    "    for i, p in enumerate(prompts[:3]):\n",
    "        image = pipe(prompt=p, num_inference_steps=4, guidance_scale=0).images[0]\n",
    "        filename = f\"generated_images/image_{i + 1}.png\"  \n",
    "        image.save(filename)\n",
    "        image_paths.append(filename)\n",
    "        \n",
    "    del pipe   \n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    janus_path = \"deepseek-ai/Janus-Pro-1B\"\n",
    "    vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(janus_path)\n",
    "    tokenizer = vl_chat_processor.tokenizer\n",
    "    vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(janus_path, trust_remote_code=True)\n",
    "    vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "    \n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        question += scenes[i] + \"\\n\"\n",
    "        \n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"<|User|>\",\n",
    "                \"content\": f\"<image_placeholder>\\n{question}\",\n",
    "                \"images\": [img_path],   # sólo la ruta\n",
    "            },\n",
    "            {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n",
    "        ]\n",
    "        \n",
    "        pil_images = load_pil_images(conversation)\n",
    "\n",
    "        \n",
    "        prepare_inputs = vl_chat_processor(\n",
    "            conversations=conversation,\n",
    "            images=pil_images,\n",
    "            force_batchify=True\n",
    "        ).to(vl_gpt.device)\n",
    "        inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "        \n",
    "        output = vl_gpt.language_model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=prepare_inputs.attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "        )[0]\n",
    "\n",
    "        \n",
    "        text = tokenizer.decode(output.cpu().tolist(), skip_special_tokens=True)\n",
    "        answers.append(text)\n",
    "\n",
    "    return {\n",
    "        \"message\": \"Proceso completado\",\n",
    "        \"generated_images\": image_paths,\n",
    "        \"answers\": answers     \n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Iniciar ngrok y servidor\n",
    "# ----------------------\n",
    "public_url = ngrok.connect(8000)\n",
    "print(\"Servidor público:\", public_url)\n",
    "uvicorn.run(app, port=8000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
